{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP++UnDDP2m03YI2JP6jjfE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niksom406/Learning_NLP/blob/main/TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53575560"
      },
      "source": [
        "## TF-IDF and N-grams\n",
        "\n",
        "\n",
        "### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "\n",
        "TF-IDF is a numerical statistic that reflects how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "*   **Term Frequency (TF):** This measures how frequently a term appears in a document. The more a word appears, the higher its TF.\n",
        "*   **Inverse Document Frequency (IDF):** This measures how important a term is. It is calculated by taking the logarithm of the total number of documents divided by the number of documents containing the term. Words that are common across many documents (like \"the\" or \"a\") will have a low IDF, while words that are unique to a few documents will have a high IDF.\n",
        "\n",
        "The TF-IDF score is the product of the TF and IDF. A high TF-IDF score indicates that a word is frequent in a document but rare in the rest of the corpus, suggesting it is a significant word for that document.\n",
        "\n",
        "### N-grams\n",
        "\n",
        "N-grams are contiguous sequences of n items from a given sample of text or speech. In the context of text processing, N-grams are sequences of words.\n",
        "\n",
        "*   **Unigrams (n=1):** Individual words (e.g., \"the\", \"quick\", \"brown\").\n",
        "*   **Bigrams (n=2):** Sequences of two words (e.g., \"the quick\", \"quick brown\").\n",
        "*   **Trigrams (n=3):** Sequences of three words (e.g., \"the quick brown\").\n",
        "\n",
        "Using N-grams in text representation techniques like TF-IDF can help capture the context and relationships between words, which can be beneficial for tasks like spam detection where the order and combination of words matter. By using bigrams (as done in the code), the model can learn that phrases like \"free entry\" or \"claim prize\" are more indicative of spam than the individual words \"free\" or \"claim\" alone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d15fd44b"
      },
      "source": [
        "### Load Dataset\n",
        "\n",
        "This cell loads the SMS Spam Collection dataset into a pandas DataFrame. The dataset is a tab-separated file with two columns: 'label' (indicating if the message is 'ham' or 'spam') and 'message' (the text of the SMS message)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4YDGchg8rkr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "messages=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/SMSSpam_Dataset/SMSSpamCollection.txt',\n",
        "                    sep='\\t',names=[\"label\",\"message\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "823e7642"
      },
      "source": [
        "### Display DataFrame\n",
        "\n",
        "This cell displays the contents of the loaded DataFrame. The output shows the first and last few rows of the DataFrame, giving an overview of the data structure and content."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "id": "nbY8BxEs8uGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9f0e057"
      },
      "source": [
        "### Data Cleaning And Preprocessing - Import Libraries\n",
        "\n",
        "This cell imports necessary libraries for data cleaning and preprocessing.\n",
        "- `re`: Regular expression module for text cleaning.\n",
        "- `nltk`: Natural Language Toolkit for various text processing tasks, including downloading stopwords.\n",
        "The code also downloads the 'stopwords' corpus from NLTK, which is a list of common words that are often removed during text preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Data Cleaning And Preprocessing\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "KIylqr8h8uJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6469c428"
      },
      "source": [
        "### Data Cleaning And Preprocessing - Initialize Tools\n",
        "\n",
        "This cell imports `stopwords` and `WordNetLemmatizer` from NLTK and initializes a `WordNetLemmatizer` object.\n",
        "- `stopwords`: A list of common English words to be removed.\n",
        "- `WordNetLemmatizer`: A tool to reduce words to their base or dictionary form (lemmatization)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordlemmatize=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "6NU4KStW8uL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ebf2212"
      },
      "source": [
        "### Data Cleaning And Preprocessing - Download WordNet\n",
        "\n",
        "This cell downloads the 'wordnet' corpus from NLTK. WordNet is a lexical database of English, and it's used by the `WordNetLemmatizer` to perform lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "zwNQQUOVAmwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acf17542"
      },
      "source": [
        "### Data Cleaning And Preprocessing - Apply Cleaning and Lemmatization\n",
        "\n",
        "This cell performs data cleaning and preprocessing on the 'message' column of the DataFrame.\n",
        "It iterates through each message, performs the following steps:\n",
        "1. Removes characters that are not letters using regular expressions.\n",
        "2. Converts the message to lowercase.\n",
        "3. Splits the message into individual words.\n",
        "4. Applies lemmatization to each word and removes stopwords.\n",
        "5. Joins the processed words back into a string.\n",
        "The cleaned and preprocessed messages are stored in the `corpus` list."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=[]\n",
        "for i in range(0,len(messages)):\n",
        "    review=re.sub('[^a-zA-z]',' ',messages['message'][i])\n",
        "    review=review.lower()\n",
        "    review=review.split()\n",
        "    review=[wordlemmatize.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
        "    review=' '.join(review)\n",
        "    corpus.append(review)"
      ],
      "metadata": {
        "id": "5t2C0Yi68uOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a696d7a4"
      },
      "source": [
        "### Display Corpus\n",
        "\n",
        "This cell displays the contents of the `corpus` list, which contains the cleaned and preprocessed SMS messages."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "id": "WMAooWPB8uRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d24c033f"
      },
      "source": [
        "### Import TfidfVectorizer\n",
        "\n",
        "This cell imports the `TfidfVectorizer` class from the `sklearn.feature_extraction.text` module. This class is used to convert a collection of raw documents to a matrix of TF-IDF features."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "AbFfcxRD9DIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d878742b"
      },
      "source": [
        "### TF-IDF Vectorization\n",
        "\n",
        "This cell creates an instance of `TfidfVectorizer` with `max_features=1000` (to consider only the top 1000 most frequent terms) and `binary=True` (to use binary term frequency). It then applies the vectorizer to the `corpus` to transform the text data into a matrix of TF-IDF features, which is stored in the variable `X`. The `.toarray()` method converts the sparse matrix output of `fit_transform` into a dense NumPy array."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidfconverter = TfidfVectorizer(max_features=1000,binary=True)\n",
        "X = tfidfconverter.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "zW6GngZV9DLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06840ed4"
      },
      "source": [
        "### Configure NumPy Print Options\n",
        "\n",
        "This cell imports the `numpy` library and configures its print options to display the full array without truncation and format floating-point numbers with 3 significant digits."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(edgeitems=30,linewidth=100000,\n",
        "                    formatter=dict(float=lambda x:\"%.3g\" % x))"
      ],
      "metadata": {
        "id": "LG6ua04R9DNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "026c2b13"
      },
      "source": [
        "### Display TF-IDF Matrix\n",
        "\n",
        "This cell displays the `X` array, which contains the TF-IDF features of the SMS messages. The output is a numerical representation of the text data, suitable for machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "Uibeufh___94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89eca998"
      },
      "source": [
        "### TF-IDF with N-grams Vectorization\n",
        "\n",
        "This cell creates an instance of `TfidfVectorizer` with `max_features=1000` and `binary=True`, but this time it also includes `ngram_range=(2, 2)`. This means the vectorizer will consider bigrams (sequences of two words) instead of individual words. It then applies the vectorizer to the `corpus` to transform the text data into a matrix of TF-IDF features based on bigrams."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(max_features=1000,binary=True,ngram_range=( 2,2))\n",
        "X = tfidf.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "j4Eh071hBA-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f50ecf07"
      },
      "source": [
        "### Display TF-IDF Vocabulary with N-grams\n",
        "\n",
        "This cell displays the vocabulary learned by the `TfidfVectorizer` when using bigrams (`ngram_range=(2, 2)`). The output is a dictionary where keys are the bigrams (sequences of two words) and values are their corresponding indices in the TF-IDF matrix."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf.vocabulary_"
      ],
      "metadata": {
        "id": "sBHq5rPFBilT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfd8b942"
      },
      "source": [
        "### Display TF-IDF Matrix with N-grams\n",
        "\n",
        "This cell displays the `X` array, which contains the TF-IDF features of the SMS messages based on bigrams. The output is a numerical representation of the text data using bigrams, which can capture more context than individual words."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "nYZDeCUQBlRv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}