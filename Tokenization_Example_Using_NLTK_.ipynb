{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBpVIUoSK6F/S/EkTd86CA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niksom406/Learning_NLP/blob/main/Tokenization_Example_Using_NLTK_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "838efe13"
      },
      "source": [
        "This notebook demonstrates different tokenization techniques using the NLTK library.\n",
        "\n",
        "**Tokenization** is the process of breaking down a piece of text into smaller units called tokens. These tokens can be words, sentences, or subword units."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39d68b26"
      },
      "source": [
        "### Installing the NLTK library\n",
        "\n",
        "First, we install the Natural Language Toolkit (NLTK) library using pip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR-_KAI0-LRC",
        "outputId": "fdcf0176-e45c-471b-839d-3241b92374f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7faa1bd2"
      },
      "source": [
        "### Defining the Corpus\n",
        "\n",
        "Here we define a sample text, which we will refer to as the \"corpus,\" for demonstrating the tokenization process."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"Hello Welcome, This is a demo to help me learn tokenization topic.\n",
        "Will be working various such projects. This is my first lesson in NLP.\n",
        "Stay Tuned in Nikita's NLP Learning Experience.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JuLChJUH-V25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47cfc187"
      },
      "source": [
        "### Printing the Corpus\n",
        "\n",
        "We print the corpus to see the original text."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnC4JSyH-nnK",
        "outputId": "7cde6570-e36f-4105-8bbe-f548e4910de5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome, This is a demo to help me learn tokenization topic. \n",
            "Will be working various such projects. This is my first lesson in NLP.\n",
            "Stay Tuned in Nikita's NLP Learning Experience.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9fe7e52"
      },
      "source": [
        "### Sentence Tokenization\n",
        "\n",
        "Sentence tokenization is the process of splitting a text into individual sentences. We use the `sent_tokenize` function from NLTK for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenization\n",
        "## Sentence --> paragraphs\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "hJHYPO8O-p26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d111753"
      },
      "source": [
        "### Downloading NLTK data\n",
        "\n",
        "Some NLTK functions, like `sent_tokenize`, require specific data packages. We download the `punkt` tokenizer models here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba7e6335",
        "outputId": "c251ea10-bbd3-48a0-f630-18345bdf8131"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dcf44e9"
      },
      "source": [
        "### Performing Sentence Tokenization\n",
        "\n",
        "We apply `sent_tokenize` to our corpus and store the resulting list of sentences in the `documents` variable."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "exXssMML-qHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e9c4e9d"
      },
      "source": [
        "### Printing the Tokenized Sentences\n",
        "\n",
        "We print the `documents` list to see the individual sentences."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXMMeDcb-qUT",
        "outputId": "541aacb9-215d-4225-b648-03215f7daafa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello Welcome, This is a demo to help me learn tokenization topic.', 'Will be working various such projects.', 'This is my first lesson in NLP.', \"Stay Tuned in Nikita's NLP Learning Experience.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQxUdepA-qol",
        "outputId": "f7e5a1b6-6827-49e3-9fd1-9aa889c5908a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4wCncyF-qx7",
        "outputId": "4248fef4-e634-459e-962e-f16e8b5cd1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome, This is a demo to help me learn tokenization topic.\n",
            "Will be working various such projects.\n",
            "This is my first lesson in NLP.\n",
            "Stay Tuned in Nikita's NLP Learning Experience.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "634bd252"
      },
      "source": [
        "### Word Tokenization\n",
        "\n",
        "Word tokenization is the process of splitting a text into individual words. NLTK provides several word tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenization\n",
        "## Paragraph --> words\n",
        "## sentence --> words\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "DSGhiP6d-q5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "745a6102"
      },
      "source": [
        "### Using `word_tokenize` on the Corpus\n",
        "\n",
        "We use the `word_tokenize` function on the entire corpus to see how it tokenizes the text into words and punctuation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g67LStpB-rBe",
        "outputId": "276d725c-3b69-4ad5-f07e-95e63ee8ef6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'This',\n",
              " 'is',\n",
              " 'a',\n",
              " 'demo',\n",
              " 'to',\n",
              " 'help',\n",
              " 'me',\n",
              " 'learn',\n",
              " 'tokenization',\n",
              " 'topic',\n",
              " '.',\n",
              " 'Will',\n",
              " 'be',\n",
              " 'working',\n",
              " 'various',\n",
              " 'such',\n",
              " 'projects',\n",
              " '.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'my',\n",
              " 'first',\n",
              " 'lesson',\n",
              " 'in',\n",
              " 'NLP',\n",
              " '.',\n",
              " 'Stay',\n",
              " 'Tuned',\n",
              " 'in',\n",
              " 'Nikita',\n",
              " \"'s\",\n",
              " 'NLP',\n",
              " 'Learning',\n",
              " 'Experience',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd0441aa"
      },
      "source": [
        "### Using `word_tokenize` on Each Sentence\n",
        "\n",
        "We iterate through the previously tokenized sentences and apply `word_tokenize` to each sentence to see word tokenization at the sentence level."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGrQv3o4-rJo",
        "outputId": "4f8f77d3-ccc7-4a41-d4a6-b62b67509163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Welcome', ',', 'This', 'is', 'a', 'demo', 'to', 'help', 'me', 'learn', 'tokenization', 'topic', '.']\n",
            "['Will', 'be', 'working', 'various', 'such', 'projects', '.']\n",
            "['This', 'is', 'my', 'first', 'lesson', 'in', 'NLP', '.']\n",
            "['Stay', 'Tuned', 'in', 'Nikita', \"'s\", 'NLP', 'Learning', 'Experience', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e28cec40"
      },
      "source": [
        "### Treebank Word Tokenization\n",
        "\n",
        "The TreebankWordTokenizer is another word tokenizer provided by NLTK. It follows the conventions of the Penn Treebank corpus."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "PnEfz2ot-zET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "536fb71f"
      },
      "source": [
        "### Creating a TreebankWordTokenizer Instance\n",
        "\n",
        "We create an instance of the `TreebankWordTokenizer`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "6tiEHUwn-ub6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c5cc0a3"
      },
      "source": [
        "### Using TreebankWordTokenizer on the Corpus\n",
        "\n",
        "We use the `tokenize` method of the `TreebankWordTokenizer` instance on the corpus. Note how it handles punctuation differently compared to `word_tokenize`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsVHZIOVEK4j",
        "outputId": "5f0d7128-45d1-47c3-d3cf-0643911fed49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'This',\n",
              " 'is',\n",
              " 'a',\n",
              " 'demo',\n",
              " 'to',\n",
              " 'help',\n",
              " 'me',\n",
              " 'learn',\n",
              " 'tokenization',\n",
              " 'topic.',\n",
              " 'Will',\n",
              " 'be',\n",
              " 'working',\n",
              " 'various',\n",
              " 'such',\n",
              " 'projects.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'my',\n",
              " 'first',\n",
              " 'lesson',\n",
              " 'in',\n",
              " 'NLP.',\n",
              " 'Stay',\n",
              " 'Tuned',\n",
              " 'in',\n",
              " 'Nikita',\n",
              " \"'s\",\n",
              " 'NLP',\n",
              " 'Learning',\n",
              " 'Experience',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}